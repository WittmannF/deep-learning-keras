{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Day-1-Video-3",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WittmannF/deep-learning-keras/blob/master/modulo-2/atividade-1-boston-housing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fDdKPdexiUjg"
      },
      "source": [
        "# Tarefa 1: Prevendo Preços das Casas de Boston\n",
        "Em sua primeira missão, você vai treinar um modelo com um conjunto de dados real. O [Boston Housing Dataset](http://lib.stat.cmu.edu/datasets/boston) contém informações recolhidas em 1978 pelo U.S Census Serviço com relação a habitação na área de Boston Mass. Seu objetivo é criar um modelo de previsão do preço de uma casa com base em 13 atributos (características). Essas características são:\n",
        "- CRIM: Esta é a taxa de criminalidade per capita por cidade\n",
        "- ZN: Esta é a proporção de terrenos residenciais zoneada para lotes maiores do que 25.000 sq.ft.\n",
        "- INDUS: Proporção de não retalhistas acres de negócios por cidade\n",
        "- CHAS: Charles River variável binária (isto é igual a 1 se limita tracto rio; 0 de outro modo)\n",
        "- NOX: nítrico concentração de óxidos de (partes por 10 milhões)\n",
        "- RM: Número médio de quartos por habitação\n",
        "- AGE: Proporção de unidades ocupadas pelos proprietários construído antes de 1940\n",
        "- DIS: distâncias ponderados para ve centros de emprego Boston\n",
        "- RAD: Índice de acessibilidade às rodovias radiais\n",
        "- TAX: taxa de propriedade de impostos Full-valor por 10000 USD\n",
        "- PTRATIO: relação professor-aluno por cidade\n",
        "- B: Calculado como 1000 (Bk - 0.63) ^ 2, onde Bk é a proporção de pessoas de ascendência Africano americano por cidade\n",
        "- LSTAT: Percentagem da população status inferior\n",
        "\n",
        "Como variável alvo, vamos usar última coluna:\n",
        "- MEDV: valor médio das casas ocupadas pelos proprietários\n",
        "\n",
        "Opcionalmente, você também pode comparar seus resultados com o [Boston Housing Data Science Contest by Kaggle](https://www.kaggle.com/c/boston-housing). Embora o concurso já terminou, você ainda pode fazer uma apresentação tardia e comparar sua pontuação com o leaderboard.\n",
        "\n",
        "## Começando\n",
        "Você terá que percorrer os 5 passos que temos visto no vídeo 4:\n",
        "1. Explorando os dados\n",
        "   - Importação de dados\n",
        "   - Compreender os dados\n",
        "2. Preparar os dados\n",
        "   - Scaling\n",
        "   - Transformando\n",
        "   - One-Hot Encoding\n",
        "   - Train teste de divisão /\n",
        "3. Desenvolver um modelo base\n",
        "4. Verificação de Previsões\n",
        "5. Melhoria de Resultados\n",
        "6. (Opcional) Compare seus resultados no Kaggle\n",
        "Os 4 primeiros passos são parcialmente implementado. Você terá que acabar com eles e também implementar a etapa 5.\n",
        "## 1. Explorando os dados\n",
        "### Importando o conjunto de dados\n",
        "O conjunto de dados Boston Housing está disponível em [keras.io/datasets](https://keras.io/datasets/). Executar a próxima célula, a fim de fazer o download e importar o conjunto de dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HDaCddq6iUjl",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from keras.datasets import boston_housing\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zs1FZjFLiUjs"
      },
      "source": [
        "Vamos explorar e compreender este conjunto de dados.\n",
        "### Compreender o conjunto de dados (Análise Exploratória de Dados)\n",
        "O conjunto de dados já foi splited em ( `X_train` e` y_train`) e teste ( `X_test` e` y_test`) subconjuntos de treinamento. Como uma revisão, o conjunto de treinamento é usado para definir a fronteira de decisão da rede neural (ou seja, treinamento do modelo), enquanto o conjunto de teste é um conjunto independente usados ​​para avaliar o quão bom ele é o modelo com dados invisíveis. Vamos obter algumas informações a partir desses conjuntos de dados. Vamos converter o conjunto de treinamento em um [Pandas Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) a fim de obter mais estatísticas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yvps8tGOiUjt",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# TODO: Number of elements in the training set\n",
        "training_len = len(y_train)\n",
        "\n",
        "# TODO: Number of elements in the test set\n",
        "test_len = len(y_test)\n",
        "\n",
        "# Show the calculated values\n",
        "print(\"There are {} houses in the training set\".format(training_len))\n",
        "print(\"There are {} houses in the test set\".format(test_len))\n",
        "\n",
        "# Convert the training set into a Pandas Dataframe in order to get more statistics:\n",
        "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']\n",
        "df = pd.DataFrame(np.c_[X_train, y_train], columns = columns)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TbrGK-1LiUjy"
      },
      "source": [
        "A última coluna foi renomeado de ** MEDV ** Para Preço ** **, a fim de fornecer mais semântica sobre o que queremos de prever. Agora, vamos obter algumas estatísticas sobre cada recurso:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ny9-7rC2iUj0",
        "colab": {}
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yb0m5VediUj7"
      },
      "source": [
        "E, finalmente, vamos visualizar algumas dessas colunas com uma matriz de dispersão usando [seaborn](https://seaborn.pydata.org/):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OqtVZhx_iUj9",
        "colab": {}
      },
      "source": [
        "import seaborn as sns # pip install seaborn\n",
        "cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'PRICE'] \n",
        "sns.pairplot(df[cols], kind='reg', plot_kws={'line_kws':{'color':'orange'}})\n",
        "# NOTE: If there's no output, run again!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TZhM97mqiUkD"
      },
      "source": [
        "No gráfico anterior, podemos verificar as tendências de alguns recursos quando comparado com os outros. Especialmente a última linha é importante, pois permite-nos verificar as tendências de algumas características com o preço no eixo y. Também é possível verificar algumas parcelas do histograma.\n",
        "> ** NOTA: ** Se a linha anterior não funcionar, é provável porque Seaborn não está instalado. Se você estiver usando o chapéu seu caso, apenas `pip instalar seaborn` em seu terminal. Se você estiver usando o Google Colab, o Seaborn já está instalado por padrão.\n",
        "## 2. Preparar os dados\n",
        "Uma vez que o conjunto de dados já está dividida em treinamento e subconjuntos de testes, só está faltando para dimensionar o conjunto de dados.\n",
        "### 2.1 Atribuição - Escala os dados de treinamento\n",
        "Dimensionar o conjunto de dados usando scaller padrão, da mesma forma como foi apresentado no vídeo 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ugnb8QwiUkE",
        "colab": {}
      },
      "source": [
        "print(\"*** Before Scaling ***\")\n",
        "print(\"Mean of each feature in the training set\\n{}\".format(np.round(np.mean(X_train, axis=0))))\n",
        "print(\"Standard deviation of each feature in the training set\\n{}\".format(np.round(np.std(X_train, axis=0))))\n",
        "\n",
        "# Function to standardize the training data\n",
        "def standardize(X, X_mean, X_std):\n",
        "    return (X-X_mean)/X_std\n",
        "\n",
        "# Get mean of each column in the training set\n",
        "X_mean = X_train.mean(axis=0)\n",
        "X_std = X_train.std(axis=0)\n",
        "\n",
        "# Apply standardization to the features in the training and test sets\n",
        "X_train = standardize(X_train, X_mean, X_std)\n",
        "X_test = standardize(X_test, X_mean, X_std)\n",
        "\n",
        "print(\"\\n*** After Scaling ***\")\n",
        "print(\"Mean of each feature in the training set\\n{}\".format(np.round(np.mean(X_train, axis=0))))\n",
        "print(\"Standard deviation of each feature in the training set\\n{}\".format(np.round(np.std(X_train, axis=0))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XB2U4JrVZl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yRCKOKbBiUkH"
      },
      "source": [
        "## 3. Desenvolver um modelo básico\n",
        "Para a função de perda, vamos usar o mesmo que é exigido no concurso Kaggle, o erro Root Mean Squared (RMSE):\n",
        "$$ RMSE = \\sqrt {\\sum_i^N {\\frac{(y_i - \\hat{y_i})^2}{N}}} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9RMz26HuiUkH",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cESn9Ol2iUkK"
      },
      "source": [
        "Agora vamos executar o primeiro e mais simples versão do nosso modelo, da mesma forma como foi feito em vídeo 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sv5MuZbuiUkM",
        "colab": {}
      },
      "source": [
        "# 0. Import keras dependencies here\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# 1. Define your base model here\n",
        "model = Sequential([\n",
        "        Dense(units=1, input_shape=(13,))\n",
        "    ])\n",
        "\n",
        "# 2. Set your optimizer and loss function here\n",
        "opt = SGD()\n",
        "model.compile(optimizer=opt,\n",
        "             loss=[root_mean_squared_error])\n",
        "\n",
        "\n",
        "# 3. Train your model\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eipuq9gXiUkf"
      },
      "source": [
        "Quando comparado com o Video 4, a forma de entrada foi aumentada para 13 uma vez que existem 13 características."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OhBdWp5_iUkh"
      },
      "source": [
        "## 4. Previsões Verificação\n",
        "Vamos verificar alguns resultados numéricos e visuais:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ssCnLu4TiUkj",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def check_predictions(model, X, y, cols):\n",
        "    print(\"\\n**** Evaluating the Test set ****\")\n",
        "    loss_test = model.evaluate(X, y)\n",
        "    print(\"Test Loss: {:.3f}\".format(loss_test))\n",
        "    \n",
        "    for column in cols:\n",
        "        y_pred = model.predict(X)\n",
        "        idx = columns.index(column)\n",
        "        plt.scatter(X[:, idx], y, c='b', alpha=0.5, label=\"True Data\")\n",
        "        plt.scatter(X[:, idx], y_pred, c='orange', alpha=0.5, label=\"Predictions\")\n",
        "        plt.xlabel(column)\n",
        "        plt.ylabel(\"Price\")\n",
        "        plt.legend(loc=0)\n",
        "        plt.show()\n",
        "    \n",
        "check_predictions(model, X_test, y_test, ['RM', 'LSTAT'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IXg_f7haiUko"
      },
      "source": [
        "Nós podemos verificar tanto numericamente e visualmente que os resultados são muito ruins. Idealmente queremos um R2 Score próximo de 1. O valor de um dos -7 indica que o modelo é muito ruim. Nós um também confirmam isso quando comparando os verdadeiros dados e previsões em duas características. Seu trabalho vai ser para melhorar esses resultados:\n",
        "## 5. TODO: Melhorar os Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxf8XAiYiUkp"
      },
      "source": [
        "Agora é a sua vez! Tente-se algumas técnicas para melhorar os resultados anteriores. Aqui estão algumas ideias:\n",
        "- Aumentar o número de camadas ocultas e funções de ativação add:\n",
        "- Como [regra de ouro](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw), você pode começar a tentar uma camada escondida com o número de neurônios nessa camada como a média dos neurônios nas camadas de entrada, mais saída.\n",
        "- Este é um exemplo com **duas camadas escondidas** com 13 neuronios na primeira camada escondida e 7 neuronios na segunda camada escondida:\n",
        "    ```\n",
        "    model = Sequential([\n",
        "    Dense(units=13, input_shape=(13,), activation='relu'),\n",
        "    Dense(units=7, activation='relu'),\n",
        "    Dense(1)\n",
        "    ])\n",
        "    ```\n",
        "\n",
        "- Aumentar o número de épocas\n",
        "- Tune a taxa de aprendizagem\n",
        "- Alterar o otimizador\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WXJGxF-miUkq",
        "colab": {}
      },
      "source": [
        "# 0. TODO: Import keras dependencies here\n",
        "from keras.models import ...\n",
        "from keras.layers import ...\n",
        "from keras.optimizers import ...\n",
        "\n",
        "# 1. TODO: Define your base model here\n",
        "model = Sequential([\n",
        "        ...\n",
        "    ])\n",
        "\n",
        "\n",
        "# 2. TODO: Set your optimizer and loss function here\n",
        "opt = ...(lr=...)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "             loss=[root_mean_squared_error])\n",
        "\n",
        "# 3. Train your model\n",
        "model.fit(X_train, y_train, epochs=...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qk7yhAoCiUkt"
      },
      "source": [
        "Agora vamos ver a previsão do conjunto de teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MVEDkdiliUku",
        "colab": {}
      },
      "source": [
        "check_predictions(model, X_test, y_test, ['RM', 'LSTAT'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sdxCR_EyiUk3"
      },
      "source": [
        "## 6. (Opcional) Enviar para Kaggle\n",
        "Embora o concurso já está concluído, você pode tentar enviar como uma apresentação tardia em Kaggle e comparar sua pontuação com outros concorrentes no [leaderboard](https://www.kaggle.com/c/boston-housing/leaderboard). Você apenas tem de clicar no botão \"Late Submission\" no [contest page](https://www.kaggle.com/c/boston-housing) e, em seguida, fazer upload de um arquivo CSV no formato de apresentação do concurso (ID e previsões).\n",
        "A fim de tentar-se, certifique-se primeiro o download do arquivo test.csv em [kaggle.com/c/boston-housing/data](https://www.kaggle.com/c/boston-housing/data) e adicioná-lo na pasta do seu projeto. Se você estiver usando Colab, você pode ter que montar Google Drive pela primeira vez."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gLL-oU_eiUk5",
        "colab": {}
      },
      "source": [
        "USING_COLAB = False\n",
        "\n",
        "if USING_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Update here with the folder of the files of your course\n",
        "    !cd '/content/drive/My Drive/course/day-1'\n",
        "    \n",
        "\n",
        "TEST_PATH = 'kaggle_boston_housing_test.csv'\n",
        "\n",
        "# Load the test.csv file\n",
        "testdf = pd.read_csv(TEST_PATH)\n",
        "testdf.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-F5ePifBiUk_"
      },
      "source": [
        "Em seguida, execute o seguinte célula, a fim de criar um arquivo de submissão CSV com base em seu modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4rlgtBMxiUk_",
        "colab": {}
      },
      "source": [
        "# Get the column with the IDs\n",
        "ids = testdf['ID'].values\n",
        "\n",
        "# Apply Standard transform in the test set\n",
        "X_test_kaggle = testdf.drop('ID', axis=1)\n",
        "X_test_kaggle_std = sc.fit_transform(X_test_kaggle)\n",
        "\n",
        "# Get predictions\n",
        "y_pred_kaggle = model.predict(X_test_kaggle_std)\n",
        "\n",
        "# Convert to a dataframe in the submission format\n",
        "submit = pd.DataFrame(np.c_[ids, y_pred_kaggle], columns=['ID', 'medv'])\n",
        "submit['ID'] = submit['ID'].astype(int)\n",
        "\n",
        "# Convert to a CSV file\n",
        "submit.to_csv('my_submission_improved_model.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jjm8X-bLiUlE"
      },
      "source": [
        "Abaixo está a minha melhor pontuação. Você pode comparar seu melhor pontuação usando o [leaderboard](https://www.kaggle.com/c/boston-housing/leaderboard):\n",
        "![screen shot 2019-02-15 at 18 14 04](https://user-images.githubusercontent.com/5733246/52882019-824f7900-314d-11e9-97d7-17e48e4a1770.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMeFDQHnmw3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}