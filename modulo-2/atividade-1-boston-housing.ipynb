{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<A href = \"https://colab.research.google.com/github/WittmannF/udemy-deep-learning/blob/master/section-2/assignment-1-boston-housing-blank.ipynb\" target =\" _parent \"> <img src =\" https://colab.research.google.com/assets/colab-badge.svg\" alt = \"Open In Colab\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDdKPdexiUjg"
   },
   "source": [
    "# Tarefa 1: Prevendo Preços de Subúrbios de Boston\n",
    "Em sua primeira missão, você vai treinar um modelo com um conjunto de dados real. O [Boston Housing Dataset](http://lib.stat.cmu.edu/datasets/boston) contém informações recolhidas em 1978 pelo U.S Census Serviço com relação a habitação na área de Boston Mass. Seu objetivo é criar um modelo de previsão do preço de uma casa com base em 13 atributos (características). Essas características são:\n",
    "- CRIM: Esta é a taxa de criminalidade per capita por cidade\n",
    "- ZN: Esta é a proporção de terrenos residenciais zoneada para lotes maiores do que\n",
    "25.000 sq.ft.\n",
    "- INDUS: Proporção de não retalhistas acres de negócios por cidade\n",
    "- CARROÇ: Charles River variável binária (isto é igual a 1 se limita tracto rio; 0 de outro modo)\n",
    "- NOX: nítrico concentração de óxidos de (partes por 10 milhões)\n",
    "- RM: Número médio de quartos por habitação\n",
    "- Idade: Proporção de unidades ocupadas pelos proprietários construído antes de 1940\n",
    "- DIS: distâncias ponderados para ve centros de emprego Boston\n",
    "- RAD: Índice de acessibilidade às rodovias radiais\n",
    "- IMPOSTO: taxa de propriedade de impostos Full-valor por 10000 USD\n",
    "- PTRATIO: relação professor-aluno por cidade\n",
    "- B: Calculado como 1000 (Bk - 0.63) ^ 2, onde Bk é a proporção de pessoas de ascendência Africano americano por cidade\n",
    "- lstat: status inferior Percentagem da população\n",
    "Como variável alvo, vamos usar última coluna:\n",
    "- MEDV: valor médio das casas ocupadas pelos proprietários\n",
    "\n",
    "Opcionalmente, você também pode comparar seus resultados com o [Boston Housing Data Science Contest by Kaggle](https://www.kaggle.com/c/boston-housing). Embora o concurso já terminou, você ainda pode fazer uma apresentação tardia e comparar sua pontuação com o leaderboard.\n",
    "## Começando\n",
    "Você terá que percorrer os 5 passos que temos visto no vídeo 4:\n",
    "1. Explorando os dados\n",
    "   - importação de dados\n",
    "   - Compreender os dados\n",
    "2. Preparar os dados\n",
    "   - Scaling\n",
    "   - Transformando\n",
    "   - One-Hot Encoding\n",
    "   - Train teste de divisão /\n",
    "3. Desenvolver um modelo básico\n",
    "4. Previsões Verificação\n",
    "5. Resultados de Melhoria\n",
    "6. (Opcional) Compare seus resultados no Kaggle\n",
    "Os 4 primeiros passos são parcialmente implementado. Você terá que acabar com eles e também implementar a etapa 5.\n",
    "## 1. Explorando os dados\n",
    "### Importando o conjunto de dados\n",
    "O conjunto de dados Boston Housing está disponível em [keras.io/datasets](https://keras.io/datasets/). Executar a próxima célula, a fim de fazer o download e importar o conjunto de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDaCddq6iUjl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs1FZjFLiUjs"
   },
   "source": [
    "Vamos explorar e compreender este conjunto de dados.\n",
    "### Compreender o conjunto de dados (Análise Exploratória de Dados)\n",
    "O conjunto de dados já foi splited em ( `X_train` e` y_train`) e teste ( `X_test` e` y_test`) subconjuntos de treinamento. Como uma revisão, o conjunto de treinamento é usado para definir a fronteira de decisão da rede neural (ou seja, treinamento do modelo), enquanto o conjunto de teste é um conjunto independente usados ​​para avaliar o quão bom ele é o modelo com dados invisíveis. Vamos obter algumas informações a partir desses conjuntos de dados. Vamos converter o conjunto de treinamento em um [Pandas Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) a fim de obter mais estatísticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yvps8tGOiUjt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Number of elements in the training set\n",
    "training_len = len(y_train)\n",
    "\n",
    "# TODO: Number of elements in the test set\n",
    "test_len = len(y_test)\n",
    "\n",
    "# Show the calculated values\n",
    "print(\"There are {} houses in the training set\".format(training_len))\n",
    "print(\"There are {} houses in the test set\".format(test_len))\n",
    "\n",
    "# Convert the training set into a Pandas Dataframe in order to get more statistics:\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']\n",
    "df = pd.DataFrame(np.c_[X_train, y_train], columns = columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbrGK-1LiUjy"
   },
   "source": [
    "A última coluna foi renomeado de ** MEDV ** Para Preço ** **, a fim de fornecer mais semântica sobre o que queremos de prever. Agora, vamos obter algumas estatísticas sobre cada recurso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ny9-7rC2iUj0"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yb0m5VediUj7"
   },
   "source": [
    "E, finalmente, vamos visualizar algumas dessas colunas com uma matriz de dispersão usando [seaborn](https://seaborn.pydata.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqtVZhx_iUj9"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'PRICE'] \n",
    "sns.pairplot(df[cols], kind='reg', plot_kws={'line_kws':{'color':'orange'}})\n",
    "# NOTE: If there's no output, run again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TZhM97mqiUkD"
   },
   "source": [
    "No gráfico anterior, podemos verificar as tendências de alguns recursos quando comparado com os outros. Especialmente a última linha é importante, pois permite-nos verificar as tendências de algumas características com o preço no eixo y. Também é possível verificar algumas parcelas do histograma.\n",
    "> ** NOTA: ** Se a linha anterior não funcionar, é provável porque Seaborn não está instalado. Se você estiver usando o chapéu seu caso, apenas `pip instalar seaborn` em seu terminal. Se você estiver usando o Google Colab, o Seaborn já está instalado por padrão.\n",
    "## 2. Preparar os dados\n",
    "Uma vez que o conjunto de dados já está dividida em treinamento e subconjuntos de testes, só está faltando para dimensionar o conjunto de dados.\n",
    "### 2.1 Atribuição - Escala os dados de treinamento\n",
    "Dimensionar o conjunto de dados usando scaller padrão, da mesma forma como foi apresentado no vídeo 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ugnb8QwiUkE"
   },
   "outputs": [],
   "source": [
    "print(\"*** Before Scaling ***\")\n",
    "print(\"Mean of each feature in the training set\\n{}\".format(np.round(np.mean(X_train, axis=0))))\n",
    "print(\"Standard deviation of each feature in the training set\\n{}\".format(np.round(np.std(X_train, axis=0))))\n",
    "\n",
    "# Function to standardize the training data\n",
    "def standardize(X, X_mean, X_std):\n",
    "    return (X-X_mean)/X_std\n",
    "\n",
    "# Get mean of each column in the training set\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "\n",
    "# Apply standardization to the features in the training and test sets\n",
    "X_train = standardize(X_train, X_mean, X_std)\n",
    "X_test = standardize(X_test, X_mean, X_std)\n",
    "\n",
    "print(\"\\n*** After Scaling ***\")\n",
    "print(\"Mean of each feature in the training set\\n{}\".format(np.round(np.mean(X_train, axis=0))))\n",
    "print(\"Standard deviation of each feature in the training set\\n{}\".format(np.round(np.std(X_train, axis=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRCKOKbBiUkH"
   },
   "source": [
    "## 3. Desenvolver um modelo básico\n",
    "Para a função de perda, vamos usar o mesmo que é exigido no concurso Kaggle, o erro Root Mean Squared (RMSE):\n",
    "$$ RMSE = \\ sqrt {\\ sum_i ^ N {(y_i - \\ chapéu {y} _i) ^ 2}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RMz26HuiUkH"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cESn9Ol2iUkK"
   },
   "source": [
    "Agora vamos executar o primeiro e mais simples versão do nosso modelo, da mesma forma como foi feito em vídeo 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sv5MuZbuiUkM"
   },
   "outputs": [],
   "source": [
    "# 0. Import keras dependencies here\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# 1. Define your base model here\n",
    "model = Sequential([\n",
    "        Dense(units=1, input_shape=(13,))\n",
    "    ])\n",
    "\n",
    "# 2. Set your optimizer and loss function here\n",
    "opt = SGD()\n",
    "model.compile(optimizer=opt,\n",
    "             loss=[root_mean_squared_error])\n",
    "\n",
    "\n",
    "# 3. Train your model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eipuq9gXiUkf"
   },
   "source": [
    "Quando comparado com o Video 4, a forma de entrada foi aumentada para 13 uma vez que existem 13 características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhBdWp5_iUkh"
   },
   "source": [
    "## 4. Previsões Verificação\n",
    "Vamos verificar alguns resultados numéricos e visuais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssCnLu4TiUkj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def check_predictions(model, X, y, cols):\n",
    "    print(\"\\n**** Evaluating the Test set ****\")\n",
    "    loss_test = model.evaluate(X, y)\n",
    "    print(\"Test Loss: {:.3f}\".format(loss_test))\n",
    "    \n",
    "    for column in cols:\n",
    "        y_pred = model.predict(X)\n",
    "        idx = columns.index(column)\n",
    "        plt.scatter(X[:, idx], y, c='b', alpha=0.5, label=\"True Data\")\n",
    "        plt.scatter(X[:, idx], y_pred, c='orange', alpha=0.5, label=\"Predictions\")\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel(\"Price\")\n",
    "        plt.legend(loc=0)\n",
    "        plt.show()\n",
    "    \n",
    "check_predictions(model, X_test, y_test, ['RM', 'LSTAT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXg_f7haiUko"
   },
   "source": [
    "Nós podemos verificar tanto numericamente e visualmente que os resultados são muito ruins. Idealmente queremos um R2 Score próximo de 1. O valor de um dos -7 indica que o modelo é muito ruim. Nós um também confirmam isso quando comparando os verdadeiros dados e previsões em duas características. Seu trabalho vai ser para melhorar esses resultados:\n",
    "## 5. Resultados de Melhoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxf8XAiYiUkp"
   },
   "source": [
    "Agora é a sua vez! Tente-se algumas técnicas para melhorar os resultados anteriores. Primeiro de tudo, vamos aprender como adicionar camadas ocultas e funções de ativação em um modelo.\n",
    "### Adicionando camadas ocultas e funções de ativação\n",
    "Aqui estão alguns exemplos de como algumas arquiteturas ilustrados de [Playground Tensorflow](https://playground.tensorflow.org/) seria programado em keras:\n",
    "Exemplo ####: Uma camada de entrada, 4 camadas ocultas (6, 5, 5 e 4 unidades com activações Relu) e uma camada de saída (activação sigmóide):\n",
    "- **Ilustração:**\n",
    "! [screen shot 2019-02-16 at 14 31 43](https://user-images.githubusercontent.com/5733246/52902488-7967b180-31f8-11e9-84a2-fcb4ffd24159.png)\n",
    "- ** Modelos em Hardware: **\n",
    "```\n",
    "model = Sequential([\n",
    "    Dense(units=6, input_shape=(2,), activation='relu')),\n",
    "    Dense(units=5, activation='relu')),\n",
    "    Dense(units=5, activation='relu')),\n",
    "    Dense(units=4, activation='relu')),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "### A melhoria da Base de Dados de Modelo\n",
    "Aqui estão algumas ideias:\n",
    "- Aumentar o número de camadas ocultas e funções de ativação add:\n",
    "- Como [rule of thumb](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw), você pode começar a tentar uma camada escondida com o número de neurônios nessa camada como a média dos neurônios nas camadas de entrada, mais saída.\n",
    "- Este é um exemplo ** duas camadas escondidas ** com 13 neurios na camada escondida primeiro e 7 unidades na segunda camada escondida. Relu função funções de activação nas duas camadas escondidas. Um neurónio na camada de saída sem função de activação (uma vez que é um problema de regressão):\n",
    "```\n",
    "modelo sequencial = ([\n",
    "Densas (unidades = 13, input_shape = (13,), activação = 'Relu'),\n",
    "Denso (unidades = 7, activação = 'Relu'),\n",
    "Densa (1)\n",
    "])\n",
    "```\n",
    "Use-o como seu ponto de partida!\n",
    "- Aumentar o número de épocas\n",
    "- Tune a taxa de aprendizagem\n",
    "- Alterar o otimizador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WXJGxF-miUkq"
   },
   "outputs": [],
   "source": [
    "# 0. TODO: Import keras dependencies here\n",
    "from keras.models import ...\n",
    "from keras.layers import ...\n",
    "from keras.optimizers import ...\n",
    "\n",
    "# 1. TODO: Define your base model here\n",
    "model = Sequential([\n",
    "        ...\n",
    "    ])\n",
    "\n",
    "\n",
    "# 2. TODO: Set your optimizer and loss function here\n",
    "opt = ...(lr=...)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "             loss=[root_mean_squared_error])\n",
    "\n",
    "# 3. Train your model\n",
    "model.fit(X_train, y_train, epochs=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qk7yhAoCiUkt"
   },
   "source": [
    "Agora vamos ver a previsão do conjunto de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVEDkdiliUku"
   },
   "outputs": [],
   "source": [
    "check_predictions(model, X_test, y_test, ['RM', 'LSTAT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdxCR_EyiUk3"
   },
   "source": [
    "## 6. (Opcional) Enviar para Kaggle\n",
    "Embora o concurso já está concluído, você pode tentar enviar como uma apresentação tardia em Kaggle e comparar sua pontuação com outros concorrentes no [leaderboard](https://www.kaggle.com/c/boston-housing/leaderboard). Você apenas tem de clicar no botão \"Late Submission\" no [contest page](https://www.kaggle.com/c/boston-housing) e, em seguida, fazer upload de um arquivo CSV no formato de apresentação do concurso (ID e previsões).\n",
    "A fim de tentar-se, certifique-se primeiro o download do arquivo test.csv em [kaggle.com/c/boston-housing/data](https://www.kaggle.com/c/boston-housing/data) e adicioná-lo na pasta do seu projeto. Se você estiver usando Colab, você pode ter que montar Google Drive pela primeira vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLL-oU_eiUk5"
   },
   "outputs": [],
   "source": [
    "USING_COLAB = False\n",
    "\n",
    "if USING_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Update here with the folder of the files of your course\n",
    "    !cd '/content/drive/My Drive/course/day-1'\n",
    "    \n",
    "\n",
    "TEST_PATH = 'kaggle_boston_housing_test.csv'\n",
    "\n",
    "# Load the test.csv file\n",
    "testdf = pd.read_csv(TEST_PATH)\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-F5ePifBiUk_"
   },
   "source": [
    "Em seguida, execute o seguinte célula, a fim de criar um arquivo de submissão CSV com base em seu modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rlgtBMxiUk_"
   },
   "outputs": [],
   "source": [
    "# Get the column with the IDs\n",
    "ids = testdf['ID'].values\n",
    "\n",
    "# Apply Standard transform in the test set\n",
    "X_test_kaggle = testdf.drop('ID', axis=1)\n",
    "X_test_kaggle_std = sc.fit_transform(X_test_kaggle)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_kaggle = model.predict(X_test_kaggle_std)\n",
    "\n",
    "# Convert to a dataframe in the submission format\n",
    "submit = pd.DataFrame(np.c_[ids, y_pred_kaggle], columns=['ID', 'medv'])\n",
    "submit['ID'] = submit['ID'].astype(int)\n",
    "\n",
    "# Convert to a CSV file\n",
    "submit.to_csv('my_submission_improved_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jjm8X-bLiUlE"
   },
   "source": [
    "Abaixo está a minha melhor pontuação. Você pode comparar seu melhor pontuação usando o [leaderboard](https://www.kaggle.com/c/boston-housing/leaderboard):\n",
    "! [screen shot 2019-02-15 at 18 14 04](https://user-images.githubusercontent.com/5733246/52882019-824f7900-314d-11e9-97d7-17e48e4a1770.png)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "include_colab_link": true,
   "name": "Day-1-Video-3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
